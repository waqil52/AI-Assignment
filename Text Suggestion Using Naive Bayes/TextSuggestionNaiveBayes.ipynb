{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSuggestionNaiveBayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_KBejwKvqZx",
        "outputId": "4db5e318-18f6-47e4-d1ff-bc4a7d99daca"
      },
      "source": [
        "import nltk\r\n",
        "from nltk import bigrams,trigrams \r\n",
        "from nltk.corpus import reuters\r\n",
        "from collections import Counter, defaultdict\r\n",
        "from gensim.test.utils import datapath\r\n",
        "from gensim.corpora import WikiCorpus\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "WikiDatasetPath = datapath('enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2')\r\n",
        "WikiSentences = WikiCorpus(WikiDatasetPath).get_texts()\r\n",
        "print(WikiSentences)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object WikiCorpus.get_texts at 0x7f0ca08cc0f8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2UPN9Pa3Iu2",
        "outputId": "fef827df-fb21-4d6b-f021-dcf73c768a01"
      },
      "source": [
        "# punkt tokenizer\r\n",
        "nltk.download('punkt')\r\n",
        "# nltk reuters dataset\r\n",
        "nltk.download('reuters')\r\n",
        "ReutersSentences  = reuters.sents()\r\n",
        "print(ReutersSentences)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.'], ...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJU1vOGRB_Aq"
      },
      "source": [
        "# calculating probabilities for every possible next words\r\n",
        "def calculateWordsProbabilities(sentenceModel):\r\n",
        "  for nextWord in sentenceModel:\r\n",
        "    nextWords = sentenceModel[nextWord]\r\n",
        "    totalWordCount = float(sum(nextWords.values()))\r\n",
        "    for previousWord in nextWords:\r\n",
        "      nextWords[previousWord] /= totalWordCount"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP-bcbnIDQHg"
      },
      "source": [
        "# calculating single word probabilities among every words\r\n",
        "def calculateSingleWordProbability(sentenceModel,wordCount):\r\n",
        "  for word in sentenceModel:\r\n",
        "    sentenceModel[word] /= wordCount"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO9Zc4xMDjBP"
      },
      "source": [
        "# converting any string to lower\r\n",
        "def convertToLower(s):\r\n",
        "  if type(s)==str:\r\n",
        "    return s.lower()\r\n",
        "  return s"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X1KJOPZDwbJ"
      },
      "source": [
        "sentenceModel4 = defaultdict(lambda: set()) # default value of model's keys is set as set\r\n",
        "sentenceModel5 = defaultdict(lambda: set())\r\n",
        "\r\n",
        "# calculating overall word count in the given sentence model\r\n",
        "def calculateWordCount(sentenceModel1,sentenceModel2,sentenceModel3,sentences):\r\n",
        "  wordCount = 0\r\n",
        "  for sentence in sentences:\r\n",
        "    for word in sentence:\r\n",
        "      wordCount += 1\r\n",
        "      sentenceModel1[word] += 1 #storing counts of each word in the first model\r\n",
        "    for previousWord2,previousWord1,nextWord in trigrams(sentence,pad_right=True,pad_left=True):\r\n",
        "      previousWord1 = convertToLower(previousWord1)\r\n",
        "      previousWord2 = convertToLower(previousWord2)\r\n",
        "      nextWord = convertToLower(nextWord)\r\n",
        "      sentenceModel2[nextWord][previousWord1] += 1 # storing count of just the previous words in case of specific word occuring\r\n",
        "      sentenceModel3[nextWord][previousWord2] += 1 # storing count of 2nd previous words in case of specific word occuring\r\n",
        "      sentenceModel4[previousWord1].add(nextWord) # adding new word based on the just previous word in the trigram to the sentence model\r\n",
        "      sentenceModel5[previousWord2].add(nextWord) # adding new word based on the 2nd previous word in the trigram to the sentence model\r\n",
        "\r\n",
        "  return wordCount"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRRajwF1QQX_",
        "outputId": "635df0e7-4b7b-44f8-d5ff-0d70d3121ee8"
      },
      "source": [
        "sentenceModel1 = defaultdict(lambda:0)\r\n",
        "sentenceModel2 = defaultdict(lambda: defaultdict(lambda:0)) # The argument will be called when we try to access a key that doesn't exist\r\n",
        "sentenceModel3 = defaultdict(lambda: defaultdict(lambda:0))\r\n",
        "\r\n",
        "WikiWordCount = calculateWordCount(sentenceModel1,sentenceModel2,sentenceModel3,WikiSentences) # counting words in wiki corpus dataset\r\n",
        "print(WikiWordCount)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "452944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2UsYIwHVyAl",
        "outputId": "4a2b110e-1d38-4363-ad59-ef04c06ff5fc"
      },
      "source": [
        "ReutersWordCount = calculateWordCount(sentenceModel1,sentenceModel2,sentenceModel3,ReutersSentences) # counting words in Reuters dataset\r\n",
        "print(ReutersWordCount)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1720917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwoPOu9tWdaM"
      },
      "source": [
        "calculateWordsProbabilities(sentenceModel2) # checking probabilities in model 2\r\n",
        "calculateWordsProbabilities(sentenceModel3) # checking probabilities in model 3\r\n",
        "\r\n",
        "totalWord = WikiWordCount + ReutersWordCount\r\n",
        "calculateSingleWordProbability(sentenceModel1,totalWord) # checking each word probability in model 1"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FsXhiyiZL05"
      },
      "source": [
        "ProbabilityWordsList = [] \r\n",
        "# getting word suggestion by placing two words\r\n",
        "def WordSuggestionsByTrigram(previousWord2,previousWord1): \r\n",
        "  for nextWord in sentenceModel4[previousWord1] & sentenceModel5[previousWord2]:\r\n",
        "    naiveBayesTrigramValue = sentenceModel1[nextWord]*sentenceModel2[nextWord][previousWord1]*sentenceModel3[nextWord][previousWord2] # using naive bayes to get the weight of the each trigram \r\n",
        "    ProbabilityWordsList.append((nextWord,naiveBayesTrigramValue)) # storing the predicted words and the weights of trigram \r\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVSH82FiaiCa",
        "outputId": "db02f97a-1bc6-4aec-f35d-b4b30b8acc04"
      },
      "source": [
        "# test of suggestions given by placing words \"i have\" \r\n",
        "WordSuggestionsByTrigram('i','have')\r\n",
        "ProbabilityWordsList.sort(key=lambda o:o[1],reverse=True)\r\n",
        "print(*ProbabilityWordsList[:10])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('not', 3.385947855674401e-07) ('been', 2.524795704851632e-07) ('no', 9.284582792204195e-08) ('talked', 7.240914088224012e-08) ('deeply', 6.272877445413316e-08) ('to', 5.211120999664473e-08) ('encouraged', 3.606486339282962e-08) ('committed', 2.872077082187158e-08) ('nothing', 2.564017843979544e-08) ('never', 2.2837426160606398e-08)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdNp58a1bRwv",
        "outputId": "378cb098-dd41-45f5-d116-4313890d56d5"
      },
      "source": [
        "while(True):\r\n",
        "    text = input(\"Enter your Sentence: \")\r\n",
        "    if text == \"\":\r\n",
        "        print(\"Stopping The Program.....\")\r\n",
        "        break\r\n",
        "    \r\n",
        "    else:\r\n",
        "        try:\r\n",
        "            ProbabilityWordsList = [] \r\n",
        "            text = text.split(\" \")\r\n",
        "            WordSuggestionsByTrigram(text[0],text[1])\r\n",
        "            ProbabilityWordsList.sort(key=lambda o:o[1],reverse=True)\r\n",
        "            print(*ProbabilityWordsList[:10])\r\n",
        "            \r\n",
        "        except:\r\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your Sentence: how will\n",
            "('proceed', 1.5090538782291351e-07) ('take', 9.567398144104542e-08) ('affect', 9.25827376927431e-08) ('come', 7.443465428810637e-08) ('interpret', 7.077092502517588e-08) ('make', 6.015915796212672e-08) ('become', 5.640078870849599e-08) ('use', 5.615341814810161e-08) ('increase', 4.462008976316627e-08) ('determine', 4.2299863233438456e-08)\n",
            "Enter your Sentence: we shall\n",
            "('fare', 4.600110126636432e-07) ('be', 3.386337237335402e-08) ('not', 2.7202118079413788e-08) ('touch', 1.6428964737987255e-08) ('probably', 7.616076368603364e-09) ('have', 6.670696472976096e-09) ('continue', 5.769111788649496e-09) ('use', 5.104856195281964e-09) ('immediately', 2.0675112263718385e-09) ('any', 1.068033779773917e-09)\n",
            "Enter your Sentence: will you\n",
            "('have', 4.5482021406655204e-08) ('need', 3.227643487147699e-08) ('saved', 2.2936837196525143e-08) ('asked', 1.739182376766841e-08) ('do', 1.5861249283525236e-08) ('cannot', 1.5856039804193656e-08) ('assess', 1.533370042212144e-08) ('consider', 1.512684470796154e-08) ('give', 1.4795326321072263e-08) ('bought', 1.4538619659492919e-08)\n",
            "Enter your Sentence: the house\n",
            "('of', 6.855559844390348e-06) ('budget', 1.0151486556445502e-06) ('ways', 9.962368676620912e-07) ('said', 6.07593228296656e-07) ('and', 5.753032985675358e-07) ('had', 3.8340093800472826e-07) ('speaker', 3.6500465657882537e-07) ('in', 3.58378514445036e-07) ('agriculture', 3.4830095141887894e-07) ('banking', 3.0047256727447937e-07)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}